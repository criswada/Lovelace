{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Youtube_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DtqGVlpvxZU",
        "colab_type": "text"
      },
      "source": [
        "## **Processamento de Linguagem Natural (NLP)**\n",
        "\n",
        "[Desafio Kaggle YouTube](https://www.kaggle.com/datasnaek/youtube-new)\n",
        "\n",
        "O dataset do Youtube trends possui variáveis textuais contendo informações promissoras que podem ajudar os modelos regressores ou classificadores.\n",
        "\n",
        "Entretanto, o texto precisa ser adequadamente limpo e processado. Empregaremos as técnicas de NLP e regex. \n",
        "\n",
        "Uma vez que o texto foi processado, aplicaremos o algoritmo de clusterização famoso em processamento textual, denominado Latent Dirichlet Allocation (LDA) sobre os títulos e as tags dos Youtube trend vídeos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlm4vKf2u_uW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "c3baa636-ff4a-4913-ddef-66df030e7156"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from re import sub\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy import asarray\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk import download\n",
        "\n",
        "from gensim import corpora, models\n",
        "import gensim\n",
        "\n",
        "from warnings import filterwarnings\n",
        "\n",
        "filterwarnings('ignore')\n",
        "download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLhRYte1JBat",
        "colab_type": "text"
      },
      "source": [
        "**Sequência de tratamento do NLP**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drrm9ieFIzx8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Geração dos tokens\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "# Carrega da lista de palavras de pouco valor semântico da língua inglesa\n",
        "en_stop = stopwords.words('english')\n",
        "\n",
        "# Criação do objeto da classe PorterStemmer\n",
        "porter_stem = PorterStemmer()\n",
        "\n",
        "# Aplicando funções de NLP\n",
        "\n",
        "def NLP_processing(text):\n",
        "  token_list = []\n",
        "  for k in text:\n",
        "    # limpeza e tokenização\n",
        "    temp = sub(r'[^\\w\\s]', ' ', k) \n",
        "    temp = sub(r'[^\\D]', ' ', temp)     \n",
        "    raw = temp.lower()\n",
        "    tokens = tokenizer.tokenize(raw)   \n",
        "\n",
        "    # remoção das stop words\n",
        "    stop_tokens = [tok for tok in tokens if not tok in en_stop]\n",
        "\n",
        "    #stematização dos tokens\n",
        "    stemmed_tokens = [porter_stem.stem(j) for j in stop_tokens]     \n",
        "    \n",
        "    # adiciona tokens na lista\n",
        "    token_list.append(stemmed_tokens)\n",
        "\n",
        "  return token_list\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXiyMNlZwOfH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "5b156bac-41f2-4e5a-ea4b-a06c9ef5557a"
      },
      "source": [
        "# Carregando dataset pre-processado dos US\n",
        "df = pd.read_csv('sentiments.csv')\n",
        "df.columns"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['video_id', 'trending_date', 'title', 'channel_title', 'category_id',\n",
              "       'publish_time', 'tags', 'views', 'likes', 'dislikes', 'comment_count',\n",
              "       'description', 'likes_perc', 'dislikes_perc', 'comment_perc', 'emotion',\n",
              "       'pop'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BlOfnJ8nylg3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f3383650-5075-472a-e636-625ac1c2f0ab"
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6436, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jbRIAOKMUTh",
        "colab_type": "text"
      },
      "source": [
        "**Título**\n",
        "\n",
        "Iremos investigar a feature title do dataset. A ideia é transformar cada título em um documento e processá-lo por NLP. \n",
        "\n",
        "Os termos tokenizados serão representados em uma outra dimensão de representação. Nesta dimensão altamente esparsa, agruparemos por LDA os documentos levando em conta ao mesmo tempo: \n",
        "- a frequência das palavras em cada documento \n",
        "- a proximidade dos documentos de acordo com essa frequência.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBGWd-hUwOcr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "055f5510-b428-4a5e-935d-692e8741306d"
      },
      "source": [
        "# Removendo registros sem texto\n",
        "#text = df.title.dropna()\n",
        "text = df.tags.dropna()\n",
        "len(text)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6351"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GyWorUCwOU2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "87adabf9-c682-47aa-9867-7e44175abfe4"
      },
      "source": [
        "# Visualização de amostras de documentos tokenizados\n",
        "\n",
        "token_list = NLP_processing(text)\n",
        "token_list[:2]"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['ellen',\n",
              "  'ellen',\n",
              "  'degener',\n",
              "  'ellen',\n",
              "  'show',\n",
              "  'ellentub',\n",
              "  'ellen',\n",
              "  'audienc',\n",
              "  'season',\n",
              "  'episod',\n",
              "  'mindi',\n",
              "  'kale',\n",
              "  'mindi',\n",
              "  'kale',\n",
              "  'babi',\n",
              "  'oprah',\n",
              "  'mindi',\n",
              "  'kale',\n",
              "  'mindi',\n",
              "  'kale',\n",
              "  'offic',\n",
              "  'mindi',\n",
              "  'kale',\n",
              "  'wrinkl',\n",
              "  'time',\n",
              "  'mindi',\n",
              "  'kale',\n",
              "  'b',\n",
              "  'j',\n",
              "  'novak',\n",
              "  'katherin',\n",
              "  'oprah',\n",
              "  'hous',\n",
              "  'ellen',\n",
              "  'fan',\n",
              "  'ellen',\n",
              "  'ticket',\n",
              "  'season',\n",
              "  'daughter',\n",
              "  'mindi',\n",
              "  'kale',\n",
              "  'daughter',\n",
              "  'bj',\n",
              "  'novak',\n",
              "  'babi',\n",
              "  'daddi',\n",
              "  'ocean',\n",
              "  'ocean',\n",
              "  'movi',\n",
              "  'offic',\n",
              "  'interview',\n",
              "  'new',\n",
              "  'funni',\n",
              "  'hilari',\n",
              "  'sandra',\n",
              "  'bullock',\n",
              "  'ann',\n",
              "  'hathaway',\n",
              "  'wrinkl',\n",
              "  'time'],\n",
              " ['megan',\n",
              "  'mullal',\n",
              "  'megan',\n",
              "  'mullal',\n",
              "  'grace',\n",
              "  'karen',\n",
              "  'grace',\n",
              "  'actress',\n",
              "  'nick',\n",
              "  'offerman',\n",
              "  'ellen',\n",
              "  'degener',\n",
              "  'ellen',\n",
              "  'degener',\n",
              "  'ellen',\n",
              "  'show',\n",
              "  'ellen',\n",
              "  'fan',\n",
              "  'ellen',\n",
              "  'ticket',\n",
              "  'ellentub',\n",
              "  'ellen',\n",
              "  'audienc',\n",
              "  'grace',\n",
              "  'karen',\n",
              "  'roomat',\n",
              "  'funni',\n",
              "  'interview',\n",
              "  'nick',\n",
              "  'offerman',\n",
              "  'wed',\n",
              "  'marriag',\n",
              "  'roomat',\n",
              "  'stori',\n",
              "  'hilari',\n",
              "  'bob',\n",
              "  'burger',\n",
              "  'emmi',\n",
              "  'hobbi',\n",
              "  'femal',\n",
              "  'naiv']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cFwNhma12ipI",
        "colab_type": "text"
      },
      "source": [
        "Uma vez que temos cada texto limpo e tratado por NLP, iremos aplicar um algoritmo de clusterização para identificar padrões nos textos."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TGMRdCkz2hKi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Transforma em um corpo de dicionário id \n",
        "dictionary = corpora.Dictionary(token_list)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94uAGw0E1n_f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# convert tokenized documents in document-term matrix\n",
        "corpus = [dictionary.doc2bow(j) for j in token_list]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLJnhvaG1n8n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# generate LDA model\n",
        "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=10, id2word=dictionary, passes=20)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8lhPCNcu1nuI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "4eeac4c9-6a1f-4ab9-bc77-2ec9b5d08179"
      },
      "source": [
        "# parâmetros LDA\n",
        "# num_topics: quantidade de tópicos (# clusters)\n",
        "# num_words: quantidade de \n",
        "\n",
        "# o número na frente da palavra indica a porcentagem que ela contribui no cluster \n",
        "print(ldamodel.print_topics(num_topics=10, num_words = 4) )"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, '0.013*\"american\" + 0.012*\"music\" + 0.011*\"vs\" + 0.011*\"full\"'), (1, '0.013*\"youtub\" + 0.011*\"react\" + 0.010*\"black\" + 0.008*\"movi\"'), (2, '0.083*\"offici\" + 0.054*\"video\" + 0.042*\"trailer\" + 0.025*\"hd\"'), (3, '0.015*\"live\" + 0.009*\"trailer\" + 0.009*\"beauti\" + 0.009*\"question\"'), (4, '0.018*\"makeup\" + 0.012*\"test\" + 0.011*\"work\" + 0.010*\"school\"'), (5, '0.015*\"make\" + 0.009*\"home\" + 0.009*\"j\" + 0.008*\"break\"'), (6, '0.029*\"first\" + 0.014*\"espn\" + 0.012*\"take\" + 0.011*\"time\"'), (7, '0.008*\"use\" + 0.008*\"stori\" + 0.008*\"falcon\" + 0.008*\"good\"'), (8, '0.011*\"battl\" + 0.011*\"grace\" + 0.010*\"made\" + 0.009*\"year\"'), (9, '0.015*\"super\" + 0.014*\"bowl\" + 0.014*\"star\" + 0.013*\"audio\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwML14L7P2o6",
        "colab_type": "text"
      },
      "source": [
        "Tags\n",
        "\n",
        "O mesmo procedimento é realizado para as tags dos vídeos do Youtube..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NQ792Vh_PS_1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "ed591d38-3e7c-4268-f0fc-f97f64d95ef4"
      },
      "source": [
        "print(ldamodel.print_topics(num_topics=10, num_words = 4) )"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, '0.040*\"news\" + 0.011*\"kardashian\" + 0.010*\"iphon\" + 0.010*\"none\"'), (1, '0.019*\"nba\" + 0.016*\"first\" + 0.014*\"game\" + 0.014*\"espn\"'), (2, '0.027*\"makeup\" + 0.016*\"food\" + 0.013*\"tutori\" + 0.012*\"challeng\"'), (3, '0.014*\"tour\" + 0.012*\"super\" + 0.012*\"news\" + 0.011*\"video\"'), (4, '0.030*\"music\" + 0.025*\"video\" + 0.024*\"cat\" + 0.013*\"song\"'), (5, '0.035*\"show\" + 0.029*\"funni\" + 0.023*\"late\" + 0.020*\"video\"'), (6, '0.020*\"anim\" + 0.015*\"star\" + 0.013*\"war\" + 0.009*\"smith\"'), (7, '0.028*\"ellen\" + 0.013*\"movi\" + 0.009*\"trailer\" + 0.008*\"degener\"'), (8, '0.027*\"dog\" + 0.010*\"christma\" + 0.008*\"video\" + 0.008*\"anim\"'), (9, '0.023*\"movi\" + 0.021*\"trailer\" + 0.012*\"netflix\" + 0.009*\"wwe\"')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HATIPn_XScHg",
        "colab_type": "text"
      },
      "source": [
        "#**Resultados**\n",
        "\n",
        "Observamos alguns padrões interessantes tanto no processamento sobre o título quanto nas tags. Os clusters confirmam o mapa de palavras mais comuns apresentado na análise exploratória e contém palavras associadas às categorias mais comuns que são Entretenimento e Música e nos canais mais visualizados. \n",
        "\n",
        "\n",
        "*   **Título**\n",
        "\n",
        "cluster: \\[\"offici\", \"video\", \"trailer\", \"hd\"] \n",
        "\n",
        "cluster: \\[\"super\", \"bowl\", \"star\", \"audio\"']\n",
        "\n",
        "cluster: \\[\"makeup\", \"test\", \"work\", \"school\"'] \n",
        "\n",
        "\n",
        "*   **Tags**\n",
        "\n",
        "cluster: \\[\"ellen\", \"movi\", \"trailer\", \"degener\"]\n",
        "\n",
        "cluster: \\[\"makeup\", \"food\", \"tutori\", \"challeng\"]\n",
        "\n",
        "cluster: \\[\"nba\", \"first\", \"game\", \"espn\"]\n",
        "\n",
        "\n",
        "Algumas regras de negócios podem ser inferidas, por exemplo, sabe-se que vídeos de maquiagem são muito populares. \n",
        "\n",
        "Entretanto o cluster indica uma frequência significativa entre os vídeos trends daqueles com termos para reviews (\"test\") e para maquiagem de uso no trabalho ou para estudantes. \n",
        "\n"
      ]
    }
  ]
}
