{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Youtube.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJDklXWe-XUw",
        "colab_type": "text"
      },
      "source": [
        "## **Processamento de Linguagem Natural**\n",
        "\n",
        "O dataset do Youtube trends possui variáveis textuais contendo informações promissoras que podem ajudar os modelos regressores ou classificadores.\n",
        "\n",
        "Entretanto, o texto precisa ser adequadamente limpo e processado. Utilizaremos muito as técnicas de NLP e regex. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hha355YwIJ6z",
        "colab_type": "code",
        "outputId": "5ac807a7-5e02-4f1d-b332-4452ce8d409b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from re import sub\n",
        "\n",
        "from numpy import asarray\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from nltk.stem import PorterStemmer \n",
        "from nltk import download\n",
        "\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from warnings import filterwarnings\n",
        "\n",
        "filterwarnings('ignore')\n",
        "download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wwRDO-hhTX0v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Limpeza de dados + Engenharia de Atributos...\n",
        "\n",
        "def remove_stopwords_and_normalize(doc_text, stopwords_hash):\n",
        "  content = []\n",
        "  stemmer = PorterStemmer()\n",
        "  for word in doc_text:\n",
        "    word_clean = word.lower().strip()\n",
        "    if(stopwords_hash.get(word_clean) == None):\n",
        "      word_clean = stemmer.stem(word_clean)\n",
        "      content.append(word_clean)  \n",
        "  return content\n",
        "\n",
        "\n",
        "def tokenizer(text):\n",
        "  token_list = []\n",
        "  tokenizer = RegexpTokenizer(r'\\w+')\n",
        "  tokens = tokenizer.tokenize(text)\n",
        "  for word in tokens:\n",
        "    if word not in token_list:\n",
        "      token_list.append(word)\n",
        "  return token_list\n",
        "\n",
        "\n",
        "def data_cleaning(news_list, target_list):\n",
        "  X_clean, Y_clean = [], []\n",
        "  stopwords_dict = {word:0 for word in stopwords.words('english')}\n",
        "  for idx, news in enumerate(news_list):\n",
        "    text = sub(r'[^\\w\\s]', ' ', news)\n",
        "    text = sub(r'[^\\D]', ' ', text)\n",
        "    text = tokenizer(text)\n",
        "    text = remove_stopwords_and_normalize(text, stopwords_dict)\n",
        "    text = ' '.join(text).strip()\n",
        "    if(len(text) > 0):\n",
        "      X_clean.append(text)\n",
        "      Y_clean.append(target_list[idx])\n",
        "  return X_clean, Y_clean\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKrk8ett-cB0",
        "colab_type": "code",
        "outputId": "857996ab-53a6-4790-b9c5-f28fc4bd82a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "df = pd.read_csv('sentiments.csv')\n",
        "df.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['video_id', 'trending_date', 'title', 'channel_title', 'category_id',\n",
              "       'publish_time', 'tags', 'views', 'likes', 'dislikes', 'comment_count',\n",
              "       'description', 'likes_perc', 'dislikes_perc', 'comment_perc', 'emotion',\n",
              "       'pop'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdLHCcQOQ_Rj",
        "colab_type": "code",
        "outputId": "6c297185-93d1-4eb4-fd1a-ae69ca06b496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "df2 = df[['tags', 'pop']].copy()\n",
        "\n",
        "df2.dropna(inplace=True)\n",
        "df2.reset_index(drop=True, inplace=True)\n",
        "df2.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6334, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEb46_Dl-b_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_text = df2.tags.copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6E-XUPcBOVe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_target = df2[['pop']].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CYEZ4zlTB6dD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X, y = data_cleaning(df_text, np.squeeze(df_target))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9sDE7o8NTq0M",
        "colab_type": "code",
        "outputId": "4aedc0bd-c471-4cc5-f021-f791e909558f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Engenharia de Atributos + Classificação de Texto....\n",
        "\n",
        "#from imblearn.over_sampling import SMOTE\n",
        "from sklearn.utils import resample\n",
        "\n",
        "X, y = asarray(X), asarray(y)\n",
        "\n",
        "kfold = StratifiedKFold(n_splits=7, random_state=42, shuffle=True)\n",
        "\n",
        "iteration = 1\n",
        "for train_index, test_index in kfold.split(X, y):\n",
        "\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    Y_train, Y_test = y[train_index], y[test_index]\n",
        "    # print(X_train.shape, X_test.shape)\n",
        "    \n",
        "    ##############################################\n",
        "    # Class balancing\n",
        "\n",
        "    X2 = pd.concat([pd.Series(X_train), pd.Series(Y_train)], axis=1)\n",
        "    X2.columns = ['tags', 'pop']\n",
        "\n",
        "    # separate minority and majority classes\n",
        "    not_pop = X2[X2['pop']==0]\n",
        "    pop = X2[X2['pop']==1]\n",
        "\n",
        "    # upsample minority\n",
        "    pop_upsampled = resample(pop,\n",
        "                          replace=True, # sample with replacement\n",
        "                          n_samples= len(not_pop), # match number in majority class\n",
        "                          random_state=27) # reproducible results\n",
        "\n",
        "    # combine majority and upsampled minority\n",
        "    upsampled = pd.concat([not_pop, pop_upsampled])\n",
        "\n",
        "    Y_train = upsampled['pop']\n",
        "    X_train = upsampled['tags']\n",
        "\n",
        "    #################################################\n",
        "    vectorizer = TfidfVectorizer(use_idf=True, ngram_range = (1,1),\\\n",
        "                     min_df = 5, max_df = 0.8) # 5 0.7\n",
        "\n",
        "    X_train = vectorizer.fit_transform(X_train) \n",
        "    X_test  = vectorizer.transform(X_test)\n",
        "\n",
        "    classifier = RandomForestClassifier(random_state=5) #min_impurity_decrease=1e-6,\n",
        "    classifier.fit(X_train, Y_train)\n",
        "    predictions = classifier.predict(X_test)\n",
        "    \n",
        "    print(f'Fold: {iteration}')\n",
        "    print(classification_report(Y_test, predictions, target_names=['less 1M','break 1M']),'\\n\\n')\n",
        "    #print(classification_report(Y_test, predictions, target_names=['loved','hated', 'polemic', 'neutral']),'\\n\\n')\n",
        "\n",
        "    iteration+=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fold: 1\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     less 1M       0.75      0.89      0.81       596\n",
            "    break 1M       0.66      0.42      0.51       310\n",
            "\n",
            "    accuracy                           0.73       906\n",
            "   macro avg       0.70      0.65      0.66       906\n",
            "weighted avg       0.72      0.73      0.71       906\n",
            " \n",
            "\n",
            "\n",
            "Fold: 2\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     less 1M       0.75      0.85      0.80       596\n",
            "    break 1M       0.61      0.46      0.53       310\n",
            "\n",
            "    accuracy                           0.72       906\n",
            "   macro avg       0.68      0.66      0.66       906\n",
            "weighted avg       0.70      0.72      0.71       906\n",
            " \n",
            "\n",
            "\n",
            "Fold: 3\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     less 1M       0.75      0.85      0.80       596\n",
            "    break 1M       0.61      0.44      0.51       309\n",
            "\n",
            "    accuracy                           0.71       905\n",
            "   macro avg       0.68      0.65      0.65       905\n",
            "weighted avg       0.70      0.71      0.70       905\n",
            " \n",
            "\n",
            "\n",
            "Fold: 4\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     less 1M       0.77      0.89      0.82       595\n",
            "    break 1M       0.70      0.47      0.56       309\n",
            "\n",
            "    accuracy                           0.75       904\n",
            "   macro avg       0.73      0.68      0.69       904\n",
            "weighted avg       0.74      0.75      0.73       904\n",
            " \n",
            "\n",
            "\n",
            "Fold: 5\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     less 1M       0.76      0.85      0.80       595\n",
            "    break 1M       0.62      0.48      0.54       309\n",
            "\n",
            "    accuracy                           0.72       904\n",
            "   macro avg       0.69      0.66      0.67       904\n",
            "weighted avg       0.71      0.72      0.71       904\n",
            " \n",
            "\n",
            "\n",
            "Fold: 6\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     less 1M       0.75      0.87      0.80       595\n",
            "    break 1M       0.63      0.45      0.53       309\n",
            "\n",
            "    accuracy                           0.72       904\n",
            "   macro avg       0.69      0.66      0.67       904\n",
            "weighted avg       0.71      0.72      0.71       904\n",
            " \n",
            "\n",
            "\n",
            "Fold: 7\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "     less 1M       0.75      0.88      0.81       595\n",
            "    break 1M       0.66      0.44      0.52       309\n",
            "\n",
            "    accuracy                           0.73       904\n",
            "   macro avg       0.70      0.66      0.67       904\n",
            "weighted avg       0.72      0.73      0.71       904\n",
            " \n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}